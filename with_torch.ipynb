{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from imgaug import augmenters as iaa\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "folder = r'C:\\Users\\Administrator\\Documents\\Computer-Vision\\fmnist'\n",
    "\n",
    "train_fmnist = datasets.FashionMNIST(folder, download = True, train=True)\n",
    "validation_fmnist = datasets.FashionMNIST(folder, download=True, train=False)\n",
    "\n",
    "image_train, label_train = train_fmnist.data, train_fmnist.targets\n",
    "image_validation, label_validation = validation_fmnist.data, validation_fmnist.targets\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        x = x.float() / 255\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        self.x, self.y = x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        return self.x[ix], self.y[ix]\n",
    "\n",
    "def DataLoaders():\n",
    "    train_dl = DataLoader(Data(image_train, label_train), batch_size = 64, shuffle = True)\n",
    "    validation_dl = DataLoader(Data(image_validation, label_validation), batch_size = 64, shuffle = True)\n",
    "    return train_dl, validation_dl\n",
    "\n",
    "train_dl, validation_dl = DataLoaders()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size = 3)\n",
    "        self.maxp1 = nn.MaxPool2d(2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3)\n",
    "        self.maxp2 = nn.MaxPool2d(2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.line1 = nn.Linear(3200, 256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.line2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxp1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxp2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.line1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.line2(x)\n",
    "        return x\n",
    "\n",
    "def compile():\n",
    "    model = Model()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr = 0.001)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                               factor = 0.5,\n",
    "                                               patience = 2,\n",
    "                                               threshold = 0.001,\n",
    "                                               verbose = True,\n",
    "                                               min_lr = 0.01,\n",
    "                                               threshold_mode = 'abs')\n",
    "    \n",
    "    return model, loss_function, optimizer, scheduler\n",
    "\n",
    "model, loss_function, optimizer, scheduler = compile()\n",
    "\n",
    "def for_training(x, y, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predicted = model(x)\n",
    "    loss = loss_function(predicted, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    accuracy = (torch.argmax(predicted, dim = 1) == y).float().mean().item()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "def for_validation(x, y, model, validation_dl, loss_function):\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = [], []\n",
    "    for x, y, in validation_dl:\n",
    "        prediction = model(x)\n",
    "        loss = loss_function(prediction, y)\n",
    "        acc = (torch.argmax(prediction, dim = 1) == y).float().mean().item()\n",
    "        val_loss.append(loss.item())\n",
    "        val_accuracy.append(acc)\n",
    "        return np.mean(val_accuracy), np.mean(val_loss)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "train_loss, train_accuracy = [], []\n",
    "for epoch in range(10):\n",
    "    epoch_loss, epoch_accuracy = [], []\n",
    "    for x, y in train_dl:\n",
    "        loss, accuracy = for_training(x, y, model, loss_function, optimizer)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_loss)\n",
    "    mean_epoch_loss = np.mean(epoch_loss)\n",
    "    mean_epoch_accuracy = np.mean(epoch_accuracy)\n",
    "    print(f'epoch {epoch+1}/{epochs} \\Train accuracy: {mean_epoch_accuracy} Train_loss: {mean_epoch_loss}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_val_accuracy, mean_val_loss = for_validation(x, y, model, validation_dl, loss_function)\n",
    "        print(f\"Validation accuracy: {mean_val_accuracy} Validation loss: {mean_val_loss}\\n\")\n",
    "    scheduler.step(mean_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 232\n",
    "img = image_train[image]/255\n",
    "img = img.view(28, 28)\n",
    "img2 = np.roll(img, 1, axis = 1)\n",
    "img3 = torch.Tensor(img2).unsqueeze(0).unsqueeze(0)\n",
    "predict = model(img3).detach().numpy()\n",
    "plt.title(validation_fmnist.classes[np.argmax(predict)])\n",
    "plt.imshow(img, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data transformations including augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 datasets\n",
    "train_data = datasets.CIFAR10(root=folder, train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10(root=folder, train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8x8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 4x4\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Model, loss, optimizer, and scheduler\n",
    "model = CNN().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (output.argmax(dim=1) == target).sum().item()\n",
    "    return total_loss / len(train_loader), correct / len(train_loader.dataset)\n",
    "\n",
    "# Testing function\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            correct += (output.argmax(dim=1) == target).sum().item()\n",
    "    return total_loss / len(test_loader), correct / len(test_loader.dataset)\n",
    "\n",
    "# Train and evaluate the model\n",
    "num_epochs = 50\n",
    "best_acc = 0\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(model, test_loader, criterion)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "          f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Object Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Region-based Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the felzenszwalb for simple Image segmentation\n",
    "from torch_snippets import *\n",
    "from skimage.segmentation import felzenszwalb\n",
    "\n",
    "img = read('Hemanvi.jpeg', 1)\n",
    "segmented = felzenszwalb(img, scale=200, min_size=100)\n",
    "subplots(\n",
    "    [img, segmented],\n",
    "    titles = ['Original Image', 'Segmented Image'],\n",
    "    sz = 5,\n",
    "    nc=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "import selectivesearch\n",
    "import torch\n",
    "\n",
    "img = read('Hemanvi.jpeg', 1)\n",
    "\n",
    "def region_proposal(img):\n",
    "    img_lbl, regions = selectivesearch.selective_search(img, scale = 200, min_size = 100)\n",
    "    image_area = np.prod(img.shape[:2])\n",
    "    candidates = []\n",
    "    for r in regions:\n",
    "        if r['rect'] in candidates: continue\n",
    "        if r['size'] < (0.05 * image_area): continue\n",
    "        if r['size'] > (1 * image_area) : continue\n",
    "        x, y, w, h = r['rect']\n",
    "        candidates.append(list(r['rect']))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "x = region_proposal(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(boxa, boxb, epsilon = 1e-5):\n",
    "    x1 = max(boxa[0], boxb[0])\n",
    "    y1 = max(boxa[1], boxb[1])\n",
    "    x2 = min(boxa[2], boxb[2])\n",
    "    y2 = min(boxa[3], boxb[3])\n",
    "    width = x2- x1\n",
    "    height = y2- y1\n",
    "    if (width<0) or (height<0): return 0.0\n",
    "    area_overlap = width * height\n",
    "    area_a = (boxa[2] - boxa[0]) * (boxa[3] - boxa[1])\n",
    "    area_b = (boxb[2] - boxb[0]) * (boxb[3] - boxb[1])\n",
    "    combined_area = area_a + area_a - area_overlap\n",
    "    iou = area_overlap / (combined_area + epsilon)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms as T, datasets, models\n",
    "from torchvision.ops import nms\n",
    "from torch_snippets import *\n",
    "import selectivesearch\n",
    "\n",
    "image_root = r'/home/zkllmt/Documents/AI Section/Datasets/Object Detection/open-images-bus-trucks/images/images'\n",
    "df_ = pd.read_csv(r'/home/zkllmt/Documents/AI Section/Datasets/Object Detection/open-images-bus-trucks/df.csv')\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, df, root = image_root, transforms = None):\n",
    "        self.df = df\n",
    "        self.root = image_root\n",
    "        self.transforms = transforms\n",
    "        self.unique_IDs = self.df['ImageID'].unique()\n",
    "    \n",
    "    def __len__(self): return len(self.unique_IDs)\n",
    "\n",
    "    def __getitem__(self, index) -> any:\n",
    "        img_id = self.unique_IDs[index]\n",
    "        file_path = f'{self.root}/{img_id}.jpg'\n",
    "        img = read(file_path, 1)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        df = self.df.copy()\n",
    "        df = df[df['ImageID'] == img_id]\n",
    "        boxes = df['XMin,YMin,XMax,YMax'.split(',')].values\n",
    "        boxes = (boxes * np.array([w,h,w,h])).astype(np.int16).tolist()\n",
    "        labels = df['LabelName'].tolist()\n",
    "        return img, boxes, labels, file_path\n",
    "data = Data(df)\n",
    "#img, boxes, labels, file_path = data[2]\n",
    "#show(img, bbs = boxes, texts=labels, text_sz=10, title = labels)\n",
    "\n",
    "def get_candidates(img):\n",
    "    lbls, regions = selectivesearch.selective_search(img, scale = 200, min_size=100)\n",
    "    candidates = []\n",
    "    img_area = np.prod(img.shape[:2])\n",
    "    for i in regions:\n",
    "        if i['rect'] in candidates: continue\n",
    "        if i['size'] < (0.05 * img_area): continue\n",
    "        if i['size'] > (1 * img_area): continue\n",
    "        x, y, w, h = i['rect']\n",
    "        candidates.append(list(i['rect']))\n",
    "    return candidates\n",
    "\n",
    "def get_iou(boxa, boxb, epsilon = 1e-5):\n",
    "    x1 = max(boxa[0], boxb[0])\n",
    "    y1 = max(boxa[1], boxb[1])\n",
    "    x2 = min(boxa[2], boxb[2])\n",
    "    y2 = min(boxa[3], boxb[3])\n",
    "    \n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "\n",
    "    if (width <0) or (height <0):\n",
    "        return 0.0\n",
    "    \n",
    "    area_overlap = width * height\n",
    "\n",
    "    area_a = (boxa[2] - boxa[0]) * (boxa[3] - boxa[1])\n",
    "    area_b = (boxb[2] - boxb[0]) * (boxb[3] - boxb[1])\n",
    "    combined = area_a + area_b - area_overlap\n",
    "    iou = area_overlap / (combined + epsilon)\n",
    "    return iou\n",
    "\n",
    "FPATHS, GTBBS, CLSS, DELTAS, IOUS, ROIS = [],[],[],[],[],[]\n",
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, (img, boxes, labels, file_path) in enumerate(data):\n",
    "    if ix == 500: break\n",
    "\n",
    "    H, W, C = img.shape\n",
    "    candidates = get_candidates(img)\n",
    "    candidates = np.array([(x, y, x+w, y+h) for x,y,w,h in candidates])\n",
    "    ious, deltas, clss, rois = [],[],[],[]\n",
    "    ious = np.array([[get_iou(candidate, _bb_) for candidate in candidates] for _bb_ in boxes]).T\n",
    "    for jx, candidate in enumerate(candidates):\n",
    "        cx, cy, cX, cY = candidate\n",
    "        candidate_ious = ious[jx]\n",
    "        best_ious_at = np.argmax(candidate_ious)\n",
    "        best_iou = candidate_ious[best_ious_at]\n",
    "        best_bb = _x, _y, _X, _Y = boxes[best_ious_at]\n",
    "        \n",
    "        if best_iou > 0.3: clss.append(labels[best_ious_at])\n",
    "        else: clss.append(\"background\")\n",
    "\n",
    "        delta = np.array([cx-_x, cy-_y, cX-_X, cY-_Y]) / np.array([W, H, W, H])\n",
    "        deltas.append(delta)\n",
    "        rois.append(candidate / np.array([W, H, W, H]))\n",
    "\n",
    "    FPATHS.append(file_path)\n",
    "    GTBBS.append(boxes)\n",
    "    CLSS.append(clss)\n",
    "    DELTAS.append(deltas)\n",
    "    IOUS.append(ious)\n",
    "    ROIS.append(rois)\n",
    "\n",
    "targets = pd.DataFrame(flatten(CLSS), columns=['Label'])\n",
    "label2target = {l:t for t, l in enumerate(targets['Label'].unique())}\n",
    "target2label = {t:l for l, t in label2target.items()}\n",
    "\n",
    "def decode(_y):\n",
    "    _, preds = _y.max(-1)\n",
    "    return preds\n",
    "\n",
    "normalize= T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "def preprocess_img(img):\n",
    "    img = torch.tensor(img).permute(2, 0, 1)\n",
    "    img = normalize(img)\n",
    "    return img.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNNDataset(Dataset):\n",
    "    def __init__(self, fpaths, rois, labels, deltas, gtbbs):\n",
    "        self.fpaths = fpaths\n",
    "        self.labels = labels\n",
    "        self.deltas = deltas\n",
    "        self.rois = rois\n",
    "        self.gtbbs = gtbbs\n",
    "        \n",
    "    def __len__(self): return len(self.fpaths)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = str(self.fpaths[ix])\n",
    "        img = read(fpath, 1)\n",
    "        H, W, C = img.shape\n",
    "        sh = np.array([W, H, W, H])\n",
    "        gtbbs = self.gtbbs[ix]\n",
    "        deltas = self.deltas[ix]\n",
    "        labels = self.labels[ix]\n",
    "        rois = self.rois[ix]\n",
    "        bbs = np.array((rois)*sh).astype(np.uint16)\n",
    "        crops = [img[y:Y, x:X] for (x,y,X,Y) in bbs]\n",
    "        return img,crops,bbs,labels,deltas,gtbbs,fpath\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input, rois, rixs, labels, deltas =[],[],[],[],[]\n",
    "        for ix in range(len(batch)):\n",
    "            image, crops, image_bbs, image_labels, image_deltas, image_gt_bbs, image_fpath = batch[ix]\n",
    "            crops = [cv2.resize(crop, (224,224)) for crop in crops]\n",
    "            crops = [preprocess_image(crop/255.)[None] for crop in crops]\n",
    "            input.extend(crops)\n",
    "            labels.extend([label2target[c] for c in image_labels])\n",
    "            deltas.extend(image_deltas)\n",
    "        input = torch.cat(input).to\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        deltas = torch.Tensor(deltas).float()\n",
    "        return input, labels, deltas\n",
    "\n",
    "\n",
    "n_train = 9*len(FPATHS)//10\n",
    "train_ds = RCNNDataset(FPATHS[:n_train], ROIS[:n_train], CLSS[:n_train], DELTAS[:n_train], GTBBS[:n_train])\n",
    "test_ds = RCNNDataset(FPATHS[n_train:], ROIS[n_train:], CLSS[n_train:], DELTAS[n_train:], GTBBS[n_train:])\n",
    "train_loader = DataLoader(train_ds, batch_size=2, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=2, collate_fn=test_ds.collate_fn, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>FASTER RCNN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from torchsummary import summary\n",
    "from torchvision import models, datasets, transforms as transforms\n",
    "from torchvision.models import vgg16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn\n",
    "import torchvision\n",
    "from glob import glob   \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "image_root = r'/home/zkllmt/Documents/AI Section/Datasets/Object Detection/open-images-bus-trucks/images/images'\n",
    "df = pd.read_csv(r'/home/zkllmt/Documents/AI Section/Datasets/Object Detection/open-images-bus-trucks/df.csv')\n",
    "\n",
    "label2target = {l:t+1 for t, l in enumerate(df['LabelName'].unique())}\n",
    "label2target['background'] = 0\n",
    "\n",
    "target2label = {t:l for l, t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "num_classes = len(label2target)\n",
    "\n",
    "def preprocess_image(img): \n",
    "    img = torch.tensor(img).permute(2, 0, 1) \n",
    "    return img.float()\n",
    "\n",
    "class Data(Dataset):\n",
    "    w, h = 224, 224\n",
    "    def __init__(self, df, image_root = image_root):\n",
    "        self.df = df\n",
    "        self.image_dir = image_root\n",
    "        self.files = glob(os.path.join(self.image_dir, '*'))\n",
    "        self.image_infos = self.df.ImageID.unique()\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        image_id = self.image_infos[ix]\n",
    "        image_path = find(image_id, self.files)\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img = np.array(img.resize((self.w, self.h), resample = Image.BILINEAR)) / 255.\n",
    "        df = self.df.copy()\n",
    "        data = df[df['ImageID'] == image_id]\n",
    "        labels = data['LabelName'].values.tolist()\n",
    "        data = data[['XMin','YMin','XMax','YMax']].values\n",
    "        data[:, [0, 2]] *= self.w\n",
    "        data[:, [1, 3]] *= self.h\n",
    "        boxes = data.astype(np.uint32).tolist()\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.Tensor(boxes).float()\n",
    "        target['labels'] = torch.Tensor([label2target[i] for i in labels]).long()\n",
    "        img = preprocess_image(img)\n",
    "        return img, target\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        return tuple(zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_infos)\n",
    "\n",
    "\n",
    "data = Data(df)\n",
    "\n",
    "trn_ids, val_ids = train_test_split(df.ImageID.unique(), test_size=0.1, random_state=99)\n",
    "trn_df, val_df = df[df['ImageID'].isin(trn_ids)], df[df['ImageID'].isin(val_ids)]\n",
    "train_ds = Data(trn_df)\n",
    "test_ds = Data(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)\n",
    "\n",
    "def get_model():\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Defining training and validation functions\n",
    "def train_batch(inputs, model, optimizer):\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image for image in input)\n",
    "    targets = [{k: v for k, v \\\n",
    "                in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses\n",
    "@torch.no_grad()\n",
    "def validate_batch(inputs, model):\n",
    "    model.train()\n",
    "#to obtain losses, model needs to be in train mode only\n",
    "#Note that here we arn't defining the model's forward method\n",
    "#hence need to work per the way the model class is defined\n",
    "    input, targets = inputs\n",
    "    input = list(image for image in input)\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses\n",
    "\n",
    "model = get_model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9,weight_decay=0.0005)\n",
    "class Logger:\n",
    "    def __init__(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epoch_logs = []\n",
    "\n",
    "    def record(self, pos, **kwargs):\n",
    "        # Display the logs as you progress\n",
    "        print(f\"Epoch {pos:.2f}: \", end=\"\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(f\"{key}: {value:.4f} \", end=\"\")\n",
    "        print()  # Newline after each record\n",
    "\n",
    "    def report_avgs(self, epoch):\n",
    "        # Report average metrics at each epoch\n",
    "        avg_logs = {key: sum([log[key] for log in self.epoch_logs]) / len(self.epoch_logs) \n",
    "                    for key in self.epoch_logs[0]}\n",
    "        print(f\"Avg metrics at epoch {epoch}: {avg_logs}\")\n",
    "\n",
    "    def plot_epochs(self, metrics):\n",
    "        # Dummy function for plotting\n",
    "        print(f\"Plotting metrics: {metrics}\")\n",
    "\n",
    "\n",
    "n_epochs = 5\n",
    "log = Logger(n_epochs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training loop\n",
    "    _n = len(train_loader)\n",
    "    for ix, inputs in enumerate(tqdm(train_loader, desc=f\"Training epoch {epoch+1}\")):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
    "        \n",
    "        pos = (epoch + (ix + 1) / _n)\n",
    "        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss.item(), trn_regr_loss=regr_loss.item(),\n",
    "                   trn_objectness_loss=loss_objectness.item(), trn_rpn_box_reg_loss=loss_rpn_box_reg.item())\n",
    "    \n",
    "    # Validation loop\n",
    "    _n = len(test_loader)\n",
    "    for ix, inputs in enumerate(tqdm(test_loader, desc=f\"Validating epoch {epoch+1}\")):\n",
    "        loss, losses = validate_batch(inputs, model)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg']]\n",
    "        \n",
    "        pos = (epoch + (ix + 1) / _n)\n",
    "        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss.item(), val_regr_loss=regr_loss.item(),\n",
    "                   val_objectness_loss=loss_objectness.item(), val_rpn_box_reg_loss=loss_rpn_box_reg.item())\n",
    "\n",
    "    if (epoch + 1) % (n_epochs // 5) == 0:\n",
    "        log.report_avgs(epoch + 1)\n",
    "\n",
    "log.plot_epochs(['trn_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>YOLO (You Only Look Once)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, checks, hub\n",
    "checks()\n",
    "\n",
    "hub.login('7872b0b7def37af38e72e525b78bef0c3c74e4facd')\n",
    "\n",
    "model = YOLO('https://hub.ultralytics.com/models/VZ8ePp9uuGbBFLLbUD2h')\n",
    "results = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SSD Object Detection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForAI_Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
