{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f637ac74cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If your gradients contain NaNs, PyTorch will stop and tell you where it happened.\n",
    "torch.autograd.set_detect_anomaly(True) # Add this to your training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Debugging Gradients with Hooks\n",
    "# Hooks let you inspect the gradients of each layer. You can attach a hook to a tensor like this:\n",
    "import torch\n",
    "\n",
    "def print_grad(grad):\n",
    "    print(grad)\n",
    "    \n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.register_hook(print_grad)\n",
    "y.backward()\n",
    "# Attach a hook to a modelâ€™s layer (model.layer_name.weight.register_hook(print_grad)) and see the gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([[-0.6871, -1.3741],\n",
      "        [-0.5729, -1.1458]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# Define a simple model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "\n",
    "# Function to print gradients\n",
    "def print_grad(grad):\n",
    "    print(\"Gradient:\", grad)\n",
    "\n",
    "# Initialize model and data\n",
    "model = SimpleNN()\n",
    "clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Attach hook to the first layer's weights\n",
    "model.fc1.weight.register_hook(print_grad)\n",
    "\n",
    "# Sample input and target\n",
    "x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "y_true = torch.tensor([[0.0, 1.0]])\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(y_pred, y_true)\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "           aten::addmm        38.97%     339.741us        60.59%     528.161us      52.816us            10  \n",
      "               aten::t        16.24%     141.595us        31.12%     271.310us      27.131us            10  \n",
      "           aten::copy_        10.63%      92.683us        10.63%      92.683us       9.268us            10  \n",
      "       aten::transpose         9.87%      86.043us        14.88%     129.715us      12.971us            10  \n",
      "          aten::linear         8.29%      72.246us       100.00%     871.717us      87.172us            10  \n",
      "          aten::expand         8.17%      71.231us        10.27%      89.546us       8.955us            10  \n",
      "      aten::as_strided         7.11%      61.987us         7.11%      61.987us       3.099us            20  \n",
      "    aten::resolve_conj         0.71%       6.191us         0.71%       6.191us       0.310us            20  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 871.717us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.profiler\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    for i in range(10):\n",
    "        model(x) # Run some model inference \n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz. Check Later for clarifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "* 1. What is an encoder in an autoencoder?\n",
    "* 2. What loss function does an autoencoder optimize for?\n",
    "* 3. How do autoencoders help in grouping similar images?\n",
    "* 4. When is a convolutional autoencoder useful?\n",
    "* 5. Why do we get non-intuitive images if we randomly sample from vector space of embeddings obtained from vanilla/convolutional autoencoders?\n",
    "* 6. What are the loss functions that VAEs optimize for?\n",
    "* 7. How do VAEs overcome the limitation of vanilla/convolutional autoencoders to generate new images?\n",
    "* 8. During an adversarial attack, why do we modify the input image pixels and not the weight values?\n",
    "* 9. In a neural style transfer, what are the losses that we optimize for?\n",
    "* 10. Why do we consider the activation of different layers and not the original image when calculating style and content loss?\n",
    "* 11. Why do we consider gram matrix loss and not the difference between images when calculating style loss?\n",
    "* 12. Why do we warp images while building a model to generate deep fakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "* 1. Encoder takes an image as input, takes the input into lower dimension and then pass it to decoder which tries to reconstruct the image.Between encoder and decoder is called the latent space. \n",
    "* 2. The loss function is called the MSELoss because it calculates the loss of continues values.\n",
    "* 3. Autoencoders help grouping images by taking them as inputs then trasnforming them into lower dimensional inputs in order to gather the most relevant data.\n",
    "* 4. Convolutional Autoencoder is useful when we want to make a model that needs to collect useful informations or features in an image in order for us to perform image manipulation.\n",
    "* 5. \n",
    "* 6. The main VAEs loss is MSELoss.\n",
    "* 7. \n",
    "* 8. We modify the input image pixels because our focus is on input and we can't modify its weights because we don't have the control in our target model. Our only choice is that we can modify the pixel however we wanted without making it too obvious in our naked eye. \n",
    "* 9. In neural style transfer, the losses that we need to define are content loss, style loss and the gram matrix loss. Gram matrix loss helps us measure how accurate our the calculation between content loss and style loss.\n",
    "* 10. We consider gram matrix because it is a multiplication of different matrix by transpose of itself. Gram matrix also helps us measure style loss easily. Without it it will be difficult to come up with another solution. \n",
    "* 11. \n",
    "* 12. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForAI_Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
